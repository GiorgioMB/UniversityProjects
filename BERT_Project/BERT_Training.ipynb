{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiorgioMB/UniversityProjects/blob/BERT-Project/BERT_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f71Z62SF0vn4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-klIJ2L2H-z"
      },
      "source": [
        "# Initialize dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmcGFWsY0ap8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertForSequenceClassification, AdamW\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egBi5Zsl2L7J"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"zeroshot/twitter-financial-news-sentiment\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Al9uGY2Pdw"
      },
      "source": [
        "Train subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0myYL8H2SD5"
      },
      "outputs": [],
      "source": [
        "train_text_data = dataset['train']['text']\n",
        "train_numeric_labels = dataset['train']['label']\n",
        "tokenized_train_data = tokenizer(train_text_data, padding=True, truncation=True, return_tensors='pt')\n",
        "train_labels = torch.tensor(train_numeric_labels)\n",
        "train_data = TensorDataset(\n",
        "    tokenized_train_data['input_ids'],\n",
        "    tokenized_train_data['attention_mask'],\n",
        "    train_labels\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2zfWO3U2UKi"
      },
      "source": [
        "Validation subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OKLuoG12WJ3"
      },
      "outputs": [],
      "source": [
        "validation_text_data = dataset['validation']['text']\n",
        "validation_numeric_labels = dataset['validation']['label']\n",
        "tokenized_validation_data = tokenizer(validation_text_data, padding=True, truncation=True, return_tensors='pt')\n",
        "validation_labels = torch.tensor(validation_numeric_labels)\n",
        "validation_dataset = TensorDataset(\n",
        "    tokenized_validation_data['input_ids'],\n",
        "    tokenized_validation_data['attention_mask'],\n",
        "    validation_labels\n",
        ")\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA83UnKV2YfV"
      },
      "source": [
        "Test subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5Ne0rRw2Z20"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(tokenized_train_data['input_ids'], tokenized_train_data['attention_mask'], train_labels)\n",
        "train_indices, test_indices = train_test_split(range(len(train_dataset)), test_size=0.2, random_state=42)\n",
        "test_dataset = torch.utils.data.Subset(train_dataset, test_indices)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8vnfDXG2pOs"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PQmd1vK2rZP"
      },
      "outputs": [],
      "source": [
        "num_labels = 3  # Number of sentiment labels (negative, positive, neutral)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "best_validation_accuracy = 0.0\n",
        "no_improvement_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFkd-NMi22aP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "hJSIVJByU7wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "BD6W_Jl8VAV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6PsORKq254H"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20  # Adjust the number of epochs as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = F.cross_entropy(outputs.logits, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_predictions = 0\n",
        "        total_samples = 0\n",
        "        for batch in validation_loader:\n",
        "            input_ids, attention_mask, batch_labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predicted_labels = outputs.logits.argmax(dim=-1)\n",
        "            correct_predictions += (predicted_labels == batch_labels).sum().item()\n",
        "            total_samples += len(batch_labels)\n",
        "\n",
        "        validation_accuracy = correct_predictions / total_samples\n",
        "\n",
        "        if validation_accuracy > best_validation_accuracy:\n",
        "            best_validation_accuracy = validation_accuracy\n",
        "            no_improvement_count = 0\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= patience:\n",
        "                print(f'Early stopping triggered at epoch {epoch}')\n",
        "                break\n",
        "    print(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xAEqDj95io9"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Tf-4706g_z"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predicted_batch_labels = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "        true_labels.extend(batch_labels.cpu().tolist())\n",
        "        predicted_labels.extend(predicted_batch_labels.cpu().tolist())\n",
        "\n",
        "true_labels = torch.tensor(true_labels)\n",
        "predicted_labels = torch.tensor(predicted_labels)\n",
        "accuracy = (true_labels == predicted_labels).sum().item() / len(true_labels)\n",
        "class_report = classification_report(true_labels, predicted_labels, target_names=[\"negative\", \"positive\", \"neutral\"])\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l4Kcqdr_5gl"
      },
      "source": [
        "# Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufV8B68t_9fw"
      },
      "outputs": [],
      "source": [
        "save_path = '/content/drive/MyDrive/models'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L__mnIqnABu3"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation"
      ],
      "metadata": {
        "id": "LzmZyosGAlPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_data = dataset['train']['text']\n",
        "train_numeric_labels = dataset['train']['label']\n",
        "tokenized_train_data = tokenizer(train_text_data, padding=True, truncation=True, return_tensors='pt')\n",
        "train_labels = np.array(train_numeric_labels)\n",
        "n_splits = 10  # Adjust as needed\n",
        "train_accuracies = []\n",
        "validation_accuracies = []\n",
        "cross_validator = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "for fold, (train_indices, validation_indices) in enumerate(cross_validator.split(tokenized_train_data['input_ids'], train_labels)):\n",
        "    print(f\"Fold {fold + 1}/{n_splits}\")\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        tokenized_train_data['input_ids'][train_indices],\n",
        "        tokenized_train_data['attention_mask'][train_indices],\n",
        "        torch.tensor(train_labels[train_indices])\n",
        "    )\n",
        "    fold_train_accuracies = []\n",
        "    fold_validation_accuracies = []\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    validation_dataset = torch.utils.data.TensorDataset(\n",
        "        tokenized_train_data['input_ids'][validation_indices],\n",
        "        tokenized_train_data['attention_mask'][validation_indices],\n",
        "        torch.tensor(train_labels[validation_indices])\n",
        "    )\n",
        "    validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=64, shuffle=False)\n",
        "    num_epochs = 20  # Adjust the number of epochs as needed\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, batch_labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = F.cross_entropy(outputs.logits, batch_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct_predictions = 0\n",
        "            total_samples = 0\n",
        "            for batch in validation_loader:\n",
        "                input_ids, attention_mask, batch_labels = batch\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                predicted_labels = outputs.logits.argmax(dim=-1)\n",
        "                correct_predictions += (predicted_labels == batch_labels).sum().item()\n",
        "                total_samples += len(batch_labels)\n",
        "            validation_accuracy = correct_predictions / total_samples\n",
        "            train_accuracy = correct_predictions / total_samples\n",
        "            fold_train_accuracies.append(train_accuracy)\n",
        "            if validation_accuracy > best_validation_accuracy:\n",
        "                best_validation_accuracy = validation_accuracy\n",
        "                no_improvement_count = 0\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "                if no_improvement_count >= patience:\n",
        "                    print(f'Early stopping triggered at epoch {epoch}')\n",
        "                    break\n",
        "    train_accuracies.append(fold_train_accuracies)\n",
        "    validation_accuracies.append(fold_validation_accuracies)\n",
        "\n",
        "##This doesn't really work but to be honest, it's unnecessary for the training\n",
        "plt.figure(figsize=(10, 6))\n",
        "for fold in range(n_splits):\n",
        "    plt.plot(range(num_epochs), train_accuracies[fold], label=f\"Fold {fold + 1} Train\")\n",
        "    plt.plot(range(num_epochs), validation_accuracies[fold], label=f\"Fold {fold + 1} Validation\")\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracies')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lJMIa0mSAp25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find best learning rate\n"
      ],
      "metadata": {
        "id": "y56FgcEKe-Ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_finder = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda _: 1)\n",
        "lr_values = []\n",
        "loss_values = []\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    print(epoch)\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, batch_labels = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = F.cross_entropy(outputs.logits, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_values.append(optimizer.param_groups[0]['lr'])\n",
        "        loss_values.append(loss.item())\n",
        "\n",
        "        lr_finder.step()\n",
        "\n",
        "\n",
        "best_lr_index = loss_values.index(min(loss_values))\n",
        "best_lr = lr_values[best_lr_index]\n",
        "print(f\"Best learning rate: {best_lr}\")"
      ],
      "metadata": {
        "id": "UT2ZHT0LfBN4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-iiObOsm1rub",
        "6-klIJ2L2H-z",
        "qFkd-NMi22aP",
        "3xAEqDj95io9",
        "7l4Kcqdr_5gl",
        "LzmZyosGAlPe",
        "y56FgcEKe-Ga"
      ],
      "gpuType": "T4",
      "mount_file_id": "1viN_i7N6OXHGXya7-A97rFjK1MLuCCMg",
      "authorship_tag": "ABX9TyM7RyxX1OlO0NY2MRikSFAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
